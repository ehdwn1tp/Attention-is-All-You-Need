{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1062e+00, -2.1809e-02,  1.2815e+00, -2.4655e-02, -1.4383e+00],\n",
       "        [-3.2144e-01, -3.7894e-01,  1.1069e+00, -1.4259e+00,  1.1254e+00],\n",
       "        [ 9.9181e-01,  1.4814e+00,  4.2419e-01,  3.5445e-01, -2.3873e-01],\n",
       "        [-9.2738e-01,  8.9821e-01, -1.8926e-03, -4.7915e-01,  3.2255e-01],\n",
       "        [ 7.0858e-02,  5.1583e-01, -2.8379e-01, -5.0446e-01, -2.4774e+00],\n",
       "        [-1.2101e+00,  1.4796e+00,  6.6068e-01,  4.2051e-01, -6.3918e-01],\n",
       "        [ 4.2488e-01,  3.5301e-01,  6.1309e-01, -1.8117e+00, -1.2350e+00],\n",
       "        [ 4.9586e-01, -3.0867e+00,  1.0141e+00, -5.5827e-01, -8.1451e-01],\n",
       "        [ 5.1150e-01, -4.9522e-01,  4.4414e-01, -6.2467e-01, -4.3491e-01],\n",
       "        [-7.1793e-01,  1.3749e+00, -1.3722e+00,  5.4599e-01,  2.1626e+00],\n",
       "        [ 3.2684e-01, -2.0026e-01, -8.7558e-01, -8.0546e-01,  9.1550e-02],\n",
       "        [ 5.6132e-01,  7.7292e-01,  8.2753e-01, -8.7579e-01,  2.9307e-01],\n",
       "        [-7.8911e-01,  1.6329e+00, -8.7239e-01, -7.1283e-01,  2.9660e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((13, 5))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukeLim\\AppData\\Local\\Temp\\ipykernel_11120\\2430103232.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3429, 0.1110, 0.4086, 0.1107, 0.0269],\n",
       "        [0.0935, 0.0883, 0.3900, 0.0310, 0.3973],\n",
       "        [0.2488, 0.4060, 0.1410, 0.1315, 0.0727],\n",
       "        [0.0676, 0.4198, 0.1707, 0.1059, 0.2361],\n",
       "        [0.2562, 0.3998, 0.1797, 0.1441, 0.0200],\n",
       "        [0.0344, 0.5061, 0.2232, 0.1755, 0.0608],\n",
       "        [0.2911, 0.2710, 0.3514, 0.0311, 0.0554],\n",
       "        [0.3008, 0.0084, 0.5050, 0.1048, 0.0811],\n",
       "        [0.3323, 0.1214, 0.3106, 0.1067, 0.1290],\n",
       "        [0.0323, 0.2616, 0.0168, 0.1142, 0.5751],\n",
       "        [0.3330, 0.1965, 0.1000, 0.1073, 0.2631],\n",
       "        [0.2201, 0.2720, 0.2873, 0.0523, 0.1683],\n",
       "        [0.0580, 0.6540, 0.0534, 0.0626, 0.1719]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukeLim\\AppData\\Local\\Temp\\ipykernel_11120\\3756014145.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  torch.nn.functional.softmax(a).sum(1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(a).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "import math\n",
    "\n",
    "# Scaled Dot-Product Attention\n",
    "def scaled_dot_product_attention(query, key, value):\n",
    "    # Q @ KT\n",
    "    attention = query @ key.T\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / math.sqrt(key.dim())\n",
    "\n",
    "    # SoftMax\n",
    "    attention = softmax(attention)\n",
    "    print(attention)\n",
    "\n",
    "    # @ V\n",
    "    attention = attention @ value\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4494e-01, 9.4450e-03, 5.3611e-02, 4.6979e-03, 1.3790e-01, 1.7811e-02,\n",
      "         1.1885e-01, 1.2154e-01, 4.8000e-02, 2.3994e-04, 7.3453e-03, 3.3182e-02,\n",
      "         2.4245e-03],\n",
      "        [1.4046e-02, 5.8936e-01, 8.7315e-03, 4.0981e-02, 3.2070e-03, 1.1796e-02,\n",
      "         6.2631e-02, 8.3720e-02, 3.8570e-02, 1.8098e-02, 2.4136e-02, 8.4265e-02,\n",
      "         2.0459e-02],\n",
      "        [1.1977e-01, 1.3117e-02, 3.7071e-01, 3.4023e-02, 6.7244e-02, 9.2233e-02,\n",
      "         5.5574e-02, 2.2880e-03, 2.7158e-02, 4.0797e-02, 1.9127e-02, 9.8909e-02,\n",
      "         5.9045e-02],\n",
      "        [1.1933e-02, 6.9997e-02, 3.8683e-02, 1.4182e-01, 3.0797e-02, 1.4608e-01,\n",
      "         4.5463e-02, 3.5127e-03, 2.0124e-02, 1.8021e-01, 3.2899e-02, 5.5996e-02,\n",
      "         2.2249e-01],\n",
      "        [6.3797e-02, 9.9761e-04, 1.3924e-02, 5.6088e-03, 7.3915e-01, 2.3409e-02,\n",
      "         1.0709e-01, 8.6573e-03, 1.3164e-02, 2.4529e-04, 8.0277e-03, 5.9294e-03,\n",
      "         1.0001e-02],\n",
      "        [2.5897e-02, 1.1533e-02, 6.0027e-02, 8.3616e-02, 7.3575e-02, 5.3814e-01,\n",
      "         2.6945e-02, 1.0043e-03, 9.4370e-03, 3.5830e-02, 6.0678e-03, 2.7226e-02,\n",
      "         1.0070e-01],\n",
      "        [8.9563e-02, 3.1736e-02, 1.8745e-02, 1.3487e-02, 1.7444e-01, 1.3965e-02,\n",
      "         4.9556e-01, 3.5495e-02, 4.1573e-02, 4.8169e-04, 1.9017e-02, 4.9957e-02,\n",
      "         1.5979e-02],\n",
      "        [2.1506e-03, 9.9607e-04, 1.8120e-05, 2.4468e-05, 3.3112e-04, 1.2221e-05,\n",
      "         8.3343e-04, 9.9331e-01, 1.9137e-03, 8.0517e-07, 2.9007e-04, 1.1687e-04,\n",
      "         3.0814e-06],\n",
      "        [1.2887e-01, 6.9629e-02, 3.2635e-02, 2.1269e-02, 7.6396e-02, 1.7425e-02,\n",
      "         1.4811e-01, 2.9037e-01, 9.0281e-02, 4.5589e-03, 4.6355e-02, 5.9402e-02,\n",
      "         1.4700e-02],\n",
      "        [2.4173e-05, 1.2260e-03, 1.8397e-03, 7.1475e-03, 5.3417e-05, 2.4826e-03,\n",
      "         6.4398e-05, 4.5845e-06, 1.7107e-04, 9.5620e-01, 1.8788e-03, 1.0900e-03,\n",
      "         2.7814e-02],\n",
      "        [3.2962e-02, 7.2830e-02, 3.8419e-02, 5.8121e-02, 7.7871e-02, 1.8727e-02,\n",
      "         1.1325e-01, 7.3567e-02, 7.7483e-02, 8.3687e-02, 1.8489e-01, 6.2476e-02,\n",
      "         1.0572e-01],\n",
      "        [8.1282e-02, 1.3880e-01, 1.0845e-01, 5.4000e-02, 3.1396e-02, 4.5868e-02,\n",
      "         1.6239e-01, 1.6180e-02, 5.4199e-02, 2.6503e-02, 3.4103e-02, 1.8796e-01,\n",
      "         5.8870e-02],\n",
      "        [2.5954e-03, 1.4727e-02, 2.8292e-02, 9.3767e-02, 2.3144e-02, 7.4138e-02,\n",
      "         2.2701e-02, 1.8643e-04, 5.8615e-03, 2.9555e-01, 2.5220e-02, 2.5728e-02,\n",
      "         3.8808e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LukeLim\\AppData\\Local\\Temp\\ipykernel_11120\\759695481.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention = softmax(attention)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6805, -0.1607,  0.8120, -0.4195, -1.2487],\n",
       "        [-0.1034, -0.2916,  0.8272, -1.1362,  0.5485],\n",
       "        [ 0.3821,  0.9675,  0.3731, -0.1425, -0.4064],\n",
       "        [-0.5085,  1.0329, -0.1868, -0.3411,  0.3358],\n",
       "        [ 0.1573,  0.4640, -0.0364, -0.5895, -2.0774],\n",
       "        [-0.7102,  1.2285,  0.3079,  0.0171, -0.4602],\n",
       "        [ 0.3611,  0.2429,  0.4859, -1.1432, -1.1712],\n",
       "        [ 0.4961, -3.0669,  1.0122, -0.5592, -0.8137],\n",
       "        [ 0.4039, -0.7184,  0.6607, -0.7165, -0.7311],\n",
       "        [-0.7153,  1.3729, -1.3332,  0.4961,  2.0779],\n",
       "        [ 0.0563,  0.2222, -0.0095, -0.6782, -0.1467],\n",
       "        [ 0.2054,  0.4904,  0.5031, -0.7406, -0.1806],\n",
       "        [-0.6318,  1.3016, -0.6543, -0.2395,  0.6650]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_dot_product_attention(a, a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devcs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
